% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/update.tidylda.R
\name{update}
\alias{update}
\alias{update.tidylda}
\title{Update a Latent Dirichlet Allocation topic model}
\usage{
update(object, ...)

\method{update}{tidylda}(
  object,
  dtm,
  iterations = NULL,
  burnin = -1,
  optimize_alpha = FALSE,
  calc_likelihood = FALSE,
  calc_r2 = FALSE,
  return_data = FALSE,
  additional_k = 0,
  phi_as_prior = FALSE,
  ...
)
}
\arguments{
\item{object}{a fitted object of class \code{tidylda}.}

\item{...}{Other arguments to be passed to \code{\link[furrr]{future_map}}}

\item{dtm}{A document term matrix or term co-occurrence matrix of class dgCMatrix.}

\item{iterations}{Integer number of iterations for the Gibbs sampler to run.}

\item{burnin}{Integer number of burnin iterations. If \code{burnin} is greater than -1,
the resulting "phi" and "theta" matrices are an average over all iterations
greater than \code{burnin}.}

\item{optimize_alpha}{Logical. Do you want to optimize alpha every iteration?
Defaults to \code{FALSE}. See 'details' of documentation for
\code{\link[textmineR]{FitLdaModel}}for more information.}

\item{calc_likelihood}{Logical. Do you want to calculate the log likelihood every iteration?
Useful for assessing convergence. Defaults to \code{FALSE}.}

\item{calc_r2}{Logical. Do you want to calculate R-squared after the model is trained?
Defaults to \code{FALSE}. This calls \code{\link[textmineR]{CalcTopicModelR2}}.}

\item{return_data}{Logical. Do you want \code{dtm} returned as part of the model object?}

\item{additional_k}{Integer number of topics to add, defaults to 0.}

\item{phi_as_prior}{Logical. Do you want to replace \code{beta} with \code{phi}
from the previous model as the prior for words over topics?}
}
\value{
Returns an S3 object of class c("tidylda").
}
\description{
Update an LDA model using collapsed Gibbs sampling.
}
\details{
\code{update} allows you to (a) update the probabilities (i.e. weights) of 
  a previously-fit model with new data or additional iterations and (b) optionally 
  use \code{phi} of a previously-fit LDA topic model as the \code{beta} prior 
  for the new model. This is tuned by setting \code{phi_as_prior = FALSE} or
  \code{phi_as_prior = TRUE} respectively.
  
  If \code{phi_as_prior = TRUE}, then the new \code{beta} is equal to \code{phi}
  from the old model. (For handling of new tokens, see below.) Then each row
  of the new \code{beta} is rescaled so that each row has the same magnitude
  as the previous model's \code{beta}.
  
  Instead of initializing token-topic assignments in the manner for new 
  models (see \code{\link[tidylda]{tidylda}}), the update initializes in 2 
  steps:
  
  First, topic-document probabilities (i.e. \code{theta}) are obtained by a 
  call to \code{\link[tidylda]{predict.tidylda}} using \code{method = "dot"}
  for the documents in \code{dtm}. Next, both \code{phi} and \code{theta} are
  passed to an internal function, \code{\link[tidylda]{initialize_topic_counts}},
  which executes a single Gibbs iteration to assign topics to tokens and 
  documents. 
  
  \code{update} handles the addition of new vocabulary by adding a flat prior 
  over new tokens. Specifically, each entry in the new prior is equal to the
  median value of \code{beta} from the old model.
  
  You can add additional topics by setting the \code{additional_k} parameter
  to an integer greater than zero. New entries to \code{alpha} have a flat 
  prior equal to the median value of \code{alpha} in the old model. (Note that
  if \code{alpha} itself is a flat prior, i.e. scalar, then the new topics have
  the same value for their prior.) New entries to \code{beta} are the average
  of all previous topics in \code{beta}.
}
\section{Methods (by class)}{
\itemize{
\item \code{tidylda}: Update method for \code{tidylda}
}}

\note{
Updates are, as of this writing, are almost-surely useful but their behaviors 
 have not been optimized or well-studied. Caveat emptor!
}
\examples{
\dontrun{
# load a document term matrix
data(nih_sample_dtm, package = "textmineR")

d1 <- nih_sample_dtm[1:50,]

d2 <- nih_sample_dtm[51:100,]

# fit a model
m <- tidylda(d1, k = 10, 
                  iterations = 200, burnin = 175)

# update an existing model by adding documents
m2 <- update(object = m,
             dtm = rbind(d1, d2),
             iterations = 200,
             burnin = 175)
             
# use an old model as a prior for a new model
m3 <- update(object = m,
             dtm = d2, # new documents only
             phi_as_prior = TRUE,
             iterations = 200,
             burnin = 175)
             
# add topics while updating a model by adding documents
m4 <- update(object = m,
             dtm = rbind(d1, d2),
             additional_k = 3,
             iterations = 200,
             burnin = 175)
             

}
}
