
@article{tidyverse, 
title = {{Welcome to the tidyverse}}, 
author = {Wickham, Hadley and Averick, Mara and Bryan, Jennifer and Chang, Winston and McGowan, Lucy D'Agostino and François, Romain and Grolemund, Garrett and Hayes, Alex and Henry, Lionel and Hester, Jim and Kuhn, Max and Pedersen, Thomas Lin and Miller, Evan and Bache, Stephan
 Milton and Müller, Kirill and Ooms, Jeroen and Robinson, David and Seidel, Dana Paige and Spinu, Vitalie and Takahashi, Kohske and Vaughan, Davis and Wilke, Claus and Woo, Kara and Yutani, Hiroaki}, 
journal = {Journal of Open Source Software}, 
doi = {10.21105/joss.01686}, 
pages = {1686}, 
number = {43}, 
volume = {4}, 
year = {2019}
}
@article{roberts2019stm, 
title = {{stm : An R Package for Structural Topic Models}}, 
author = {Roberts, Margaret E and Stewart, Brandon M and Tingley, Dustin}, 
journal = {Journal of Statistical Software}, 
doi = {10.18637/jss.v091.i02}, 
number = {2}, 
volume = {91}, 
year = {2019}
}
@article{tidytextjss, 
title = {{Text Mining with R: A Tidy Approach}}, 
author = {Fay, Colin}, 
journal = {Journal of Statistical Software}, 
doi = {10.18637/jss.v083.b01}, 
number = {Book Review 1}, 
volume = {83}, 
year = {2018}
}
@article{quanteda, 
title = {{quanteda: An R package for the quantitative analysis of textual data}}, 
author = {Benoit, Kenneth and Watanabe, Kohei and Wang, Haiyan and Nulty, Paul and Obeng, Adam and Müller, Stefan and Matsuo, Akitaka}, 
journal = {Journal of Open Source Software}, 
doi = {10.21105/joss.00774}, 
pages = {774}, 
number = {30}, 
volume = {3}, 
year = {2018}
}
@article{tidytextjoss, 
title = {{tidytext: Text Mining and Analysis Using Tidy Data Principles in R}}, 
author = {Silge, Julia and Robinson, David}, 
journal = {The Journal of Open Source Software}, 
doi = {10.21105/joss.00037}, 
pages = {37}, 
number = {3}, 
volume = {1}, 
year = {2016}
}

@misc{chang2015lda, 
author = {Chang, Jonathan}, 
title = {{lda}}, 
url = {https://CRAN.R-project.org/package=lda}, 
copyright = {LGPL-2, LGPL-3}, 
doi = {10.32614/CRAN.package.lda},
year = {2015}
}
@article{broom, 
title = {{broom: An R Package for Converting Statistical Analysis Objects Into Tidy Data Frames}}, 
author = {Robinson, David}, 
doi = {10.32614/CRAN.package.broom},
journal = {arXiv}, 
eprint = {1412.3565}, 
abstract = {{The concept of "tidy data" offers a powerful framework for structuring data to ease manipulation, modeling and visualization. However, most R functions, both those built-in and those found in third-party packages, produce output that is not tidy, and that is therefore difficult to reshape, recombine, and otherwise manipulate. Here I introduce the broom package, which turns the output of model objects into tidy data frames that are suited to further analysis, manipulation, and visualization with input-tidy tools. Broom defines the "tidy", "augment" and "glance" generics, which arrange a model into three levels of tidy output respectively: the component level, the observation level, and the model level. I provide examples to demonstrate how these generics work with tidy tools to allow analysis and modeling of data that is divided into subsets, to recombine results from bootstrap replicates, and to perform simulations that investigate the effect of varying input parameters.}}, 
year = {2014}
}
@book{rlang, 
title = {{R: A Language and Environment for Statistical Computing}}, 
author = {Team, R Core}, 
url = {http://www.R-project.org/}, 
address = {Vienna, Austria}, 
year = {2013}
}

@inproceedings{gensim, 
author = {rek, Radim Reh u and Sojka, Petr}, 
title = {{Software Framework for Topic Modelling with Large Corpora}}, 
booktitle = {Proceedings of the LREC 2010 Workshop on New
           Challenges for NLP Frameworks}, 
pages = {45---50}, 
publisher = {ELRA}, 
address = {Valletta, Malta}, 
language = {English}, 
note = {\textbackslashurlhttp://is.muni.cz/publication/884893/en}, 
year = {2010}
}

@unpublished{mallet, 
author = {McCallum, Andrew Kachites}, 
title = {{MALLET: A Machine Learning for Language Toolkit}}, 
note = {http://mallet.cs.umass.edu}, 
year = {2002}
}
@misc{cran, 
title = {{The Comprehensive R Archive Network}}, 
author = {Team, The CRAN}, 
url = {https://cran.r-project.org/}, 
doi = {10.1002/wics.1212},
urldate = {2020-01-01}
}
@article{blei2002lda, 
title = {{Latent Dirichlet Allocation}}, 
author = {Blei, David M. and Ng, Andrew Y. and Jordan, Michael I.}, 
journal = {Journal of Machine Learning Research}, 
volume = {3}, 
year = {2003}
}

@phdthesis{jones2023latent,
  title={{Latent Dirichlet Allocation for Natural Language Statistics}},
  author={Tommy Jones},
  year={2023},
  school={George Mason University}
}

@online{tidymodelsbook,
  author = {Khun, Max},
  title = {{Conventions for R Modeling Packages}},
  year = {2019},
  url = {https://tidymodels.github.io/model-implementation-principles/index.html},
  urldate = {2022-04-12}
}

@article{wickham2014tidy,
  title={{Tidy data}},
  author={Wickham, Hadley and others},
  journal={Journal of Statistical Software},
  volume={59},
  number={10},
  pages={1--23},
  year={2014}
}

@online{tidymodels,
  author = {Max Khun and Hadley Wickham},
  title = {Tidymodels},
  doi = {10.32614/CRAN.package.tidymodels},
  year = 2018,
  urldate = {2021-01-01},
  url = {https://www.tidymodels.org/}
}

@misc{text2vec, 
author = {Selivanov, Dmitriy and Bickel, Manuel and Wang, Qing}, 
title = {{text2vec}}, 
url = {https://CRAN.R-project.org/package=text2vec}, 
publisher = {CRAN}, 
copyright = {GPL-3}, 
doi = {10.32614/CRAN.package.text2vec},
year = {2020}
}

@inproceedings{roberts2013stm, 
author = {Roberts, Margaret E. and Stewart, Brandon M. and Tingley, Dustin and Airoldi, Edoardo M.}, 
title = {{The Structural Topic Model and Applied Social Science}}, 
doi = {10.32614/CRAN.package.stm},
local-url = {file://localhost/Users/tommy/Google%20Drive/gmu/_dissertation/references/download%202020-06-23/stmnips2013.pdf}, 
year = {2013}
}

@misc{textminer, 
author = {Jones, Tommy}, 
editora = {William Doane}, 
title = {{textmineR: Functions for Text Mining and Topic Modeling}}, 
url = {https://CRAN.R-project.org/package=textmineR}, 
year = {2015}
}


@article{topicmodelspackage, 
title = {{topicmodels:  An R Package for Fitting Topic Models}}, 
author = {Grün, Bettina and Hornik, Kurt}, 
journal = {Journal of Statistical Software}, 
number = {13}, 
volume = {40}, 
local-url = {file://localhost/Users/tommy/Google%20Drive/gmu/_dissertation/references/2021-01-01/topicmodels.pdf}, 
doi = {10.32614/CRAN.package.topicmodel},
year = {2011}
}

@article{tmjss, 
title = {{Text Mining Infrastructure in R}}, 
author = {Feinerer, Ingo and Hornik, Kurt and Meyer, David}, 
journal = {Journal of Statistical Software}, 
number = {5}, 
volume = {25}, 
doi = {10.32614/CRAN.package.tm},
local-url = {file://localhost/Users/tommy/Google%20Drive/gmu/_dissertation/references/2021-01-01/textmining.pdf}, 
year = {2008}
}

@article{blei2007ctm, 
title = {{A correlated topic model of Science}}, 
author = {Blei, David M. and Lafferty, John D.}, 
journal = {The Annals of Applied Statistics}, 
issn = {1932-6157}, 
doi = {10.1214/07-aoas114}, 
abstract = {{Topic models, such as latent Dirichlet allocation (LDA), can be useful tools for the statistical analysis of document collections and other discrete data. The LDA model assumes that the words of each document arise from a mixture of topics, each of which is a distribution over the vocabulary. A limitation of LDA is the inability to model topic correlation even though, for example, a document about genetics is more likely to also be about disease than X-ray astronomy. This limitation stems from the use of the Dirichlet distribution to model the variability among the topic proportions. In this paper we develop the correlated topic model (CTM), where the topic proportions exhibit correlation via the logistic normal distribution [J. Roy. Statist. Soc. Ser. B 44 (1982) 139–177]. We derive a fast variational inference algorithm for approximate posterior inference in this model, which is complicated by the fact that the logistic normal is not conjugate to the multinomial. We apply the CTM to the articles from Science published from 1990–1999, a data set that comprises 57M words. The CTM gives a better fit of the data than LDA, and we demonstrate its use as an exploratory tool of large document collections.}}, 
pages = {17--35}, 
number = {1}, 
volume = {1}, 
local-url = {file://localhost/Users/tommy/Google%20Drive/gmu/_dissertation/references/download%202020-06-23/euclid.aoas.1183143727.pdf}, 
year = {2007}
}

@article{chen2015warplda, 
title = {{WarpLDA: a Cache Efficient O(1) Algorithm for Latent Dirichlet Allocation}}, 
author = {Chen, Jianfei and Li, Kaiwei and Zhu, Jun and Chen, Wenguang}, 
journal = {arXiv}, 
eprint = {1510.08628}, 
abstract = {{Developing efficient and scalable algorithms for Latent Dirichlet Allocation (LDA) is of wide interest for many applications. Previous work has developed an O(1) Metropolis-Hastings sampling method for each token. However, the performance is far from being optimal due to random accesses to the parameter matrices and frequent cache misses. In this paper, we first carefully analyze the memory access efficiency of existing algorithms for LDA by the scope of random access, which is the size of the memory region in which random accesses fall, within a short period of time. We then develop WarpLDA, an LDA sampler which achieves both the best O(1) time complexity per token and the best O(K) scope of random access. Our empirical results in a wide range of testing conditions demonstrate that WarpLDA is consistently 5-15x faster than the state-of-the-art Metropolis-Hastings based LightLDA, and is comparable or faster than the sparsity aware F+LDA. With WarpLDA, users can learn up to one million topics from hundreds of millions of documents in a few hours, at an unprecedentedly throughput of 11G tokens per second.}}, 
local-url = {file://localhost/Users/tommy/Google%20Drive/gmu/_dissertation/references/download%202020-06-22/1510.08628.pdf}, 
doi = {10.14778/2977797.297780},
year = {2015}
}

